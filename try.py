#!/usr/bin/env python3
"""
multimodal_rag.py

- Load Confluence-like JSON produced earlier (text paragraphs, tables, images metadata)
- Build two FAISS vector indexes:
    1) text_index (paragraph-level)
    2) table_index (each table row as a doc)
- On query:
    - retrieve top-k from both indexes
    - construct a retrieval context
    - generate an answer via a quantized local LLM (via llama-cpp-python if available)
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from tqdm import tqdm

# ------------------ CONFIG ------------------
JSON_PATH = "/mnt/data/extracted_page_html.json"   # update if needed
EMBED_MODEL_NAME = "all-MiniLM-L6-v2"             # small, fast CPU embedding
INDEX_DIR = Path("/mnt/data/rag_indexes")
INDEX_DIR.mkdir(parents=True, exist_ok=True)

TEXT_INDEX_PATH = INDEX_DIR / "text_index.faiss"
TABLE_INDEX_PATH = INDEX_DIR / "table_index.faiss"
METADATA_PATH = INDEX_DIR / "metadata.json"       # store metadata for each vector

# Generation model config (preferred: llama-cpp-python + quantized GGML)
LLAMA_CPP_MODEL_PATH = os.environ.get("LLAMA_CPP_MODEL_PATH", "/path/to/ggml-model-q4.bin")
USE_LLAMA_CPP = Path(LLAMA_CPP_MODEL_PATH).exists()

# Retrieval & response settings
TOP_K_TEXT = 5
TOP_K_TABLE = 5
EMBED_BATCH = 64
# --------------------------------------------

def load_extracted_json(json_path: str) -> Dict:
    with open(json_path, "r", encoding="utf-8") as f:
        return json.load(f)

def extract_paragraphs_and_table_rows(payload: Dict) -> Tuple[List[str], List[Dict]]:
    """
    Returns:
      - paragraphs: list[str]  (each an extracted paragraph)
      - table_rows: list[dict] with fields: {'row_text': str, 'table_page': int (optional), 'raw_html': str}
    """
    paragraphs = []
    table_rows = []

    # 1) Try to parse body.storage.value as HTML-like text (it is generated by extraction)
    body = payload.get("body", {}).get("storage", {}).get("value", "")
    # We used simple <h1>, <h2>, <p>, <table> etc. For robust parsing, use a simple heuristic split.
    # We'll split by <h2>, <h3>, <p> and also parse table HTML blocks.
    import re
    # Extract table blocks
    table_pattern = re.compile(r"(<table.*?>.*?</table>)", flags=re.DOTALL|re.IGNORECASE)
    tables = table_pattern.findall(body)
    # remove tables from body to avoid duplication
    body_no_tables = table_pattern.sub(" ", body)

    # Paragraphs: find <p>...</p> and heading tags
    para_pattern = re.compile(r"<(p|h[1-6])[^>]*>(.*?)</\1>", flags=re.DOTALL|re.IGNORECASE)
    for m in para_pattern.finditer(body_no_tables):
        txt = re.sub(r"<[^>]+>", "", m.group(2)).strip()
        if txt:
            paragraphs.append(txt)

    # Table rows: parse each table's rows
    row_pattern = re.compile(r"<tr>(.*?)</tr>", flags=re.DOTALL|re.IGNORECASE)
    cell_pattern = re.compile(r"<t[dh][^>]*>(.*?)</t[dh]>", flags=re.DOTALL|re.IGNORECASE)
    for t_idx, t_html in enumerate(tables):
        trs = row_pattern.findall(t_html)
        # assume first row is header if <th> present
        headers = []
        if trs:
            # check first row for headers
            first = trs[0]
            header_cells = re.findall(r"<th[^>]*>(.*?)</th>", first, flags=re.DOTALL|re.IGNORECASE)
            if header_cells:
                headers = [re.sub(r"<[^>]+>", "", hc).strip() for hc in header_cells]
                data_rows = trs[1:]
            else:
                data_rows = trs
            for r in data_rows:
                cells = cell_pattern.findall(r)
                cells = [re.sub(r"<[^>]+>", "", c).strip() for c in cells]
                row_text = " | ".join(cells)
                if row_text.strip():
                    table_rows.append({
                        "row_text": row_text,
                        "table_index": t_idx,
                        "raw_html": r
                    })
    return paragraphs, table_rows

# --------- Embedding utils ----------
def embed_texts(model: SentenceTransformer, texts: List[str], batch_size: int = 64) -> np.ndarray:
    embs = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        e = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)
        embs.append(e)
    if embs:
        return np.vstack(embs)
    return np.zeros((0, model.get_sentence_embedding_dimension()), dtype=np.float32)

def build_faiss_index(embeddings: np.ndarray, normalize: bool = True) -> faiss.Index:
    # Normalize to use inner product for cosine similarity
    d = embeddings.shape[1]
    if normalize:
        faiss.normalize_L2(embeddings)
        index = faiss.IndexFlatIP(d)
    else:
        index = faiss.IndexFlatL2(d)
    index.add(embeddings)
    return index

# ---------- Main pipeline ----------
def build_indexes(json_path: str):
    print("Loading JSON...")
    payload = load_extracted_json(json_path)
    paragraphs, table_rows = extract_paragraphs_and_table_rows(payload)
    print(f"Found {len(paragraphs)} paragraphs and {len(table_rows)} table rows.")

    # Load embedding model
    print("Loading embedding model:", EMBED_MODEL_NAME)
    embed_model = SentenceTransformer(EMBED_MODEL_NAME)

    # Embed paragraphs
    print("Embedding paragraphs...")
    paragraph_texts = paragraphs
    p_emb = embed_texts(embed_model, paragraph_texts, batch_size=EMBED_BATCH).astype("float32")
    if p_emb.shape[0] == 0:
        print("No paragraph embeddings produced.")
    else:
        print("Paragraph embeddings shape:", p_emb.shape)

    # Embed table rows
    print("Embedding table rows...")
    table_texts = [r["row_text"] for r in table_rows]
    t_emb = embed_texts(embed_model, table_texts, batch_size=EMBED_BATCH).astype("float32")
    if t_emb.shape[0] == 0:
        print("No table embeddings produced.")
    else:
        print("Table embeddings shape:", t_emb.shape)

    # Build FAISS indices with normalized vectors (cosine via inner product)
    if p_emb.shape[0] > 0:
        print("Building text index...")
        text_index = build_faiss_index(p_emb, normalize=True)
        faiss.write_index(text_index, str(TEXT_INDEX_PATH))
    else:
        text_index = None

    if t_emb.shape[0] > 0:
        print("Building table index...")
        table_index = build_faiss_index(t_emb, normalize=True)
        faiss.write_index(table_index, str(TABLE_INDEX_PATH))
    else:
        table_index = None

    # Save metadata (so we can map indices back to paragraphs/table rows)
    metadata = {
        "paragraphs": paragraph_texts,
        "table_rows": table_rows,
        "embed_dim": p_emb.shape[1] if p_emb.shape[0] > 0 else (t_emb.shape[1] if t_emb.shape[0] > 0 else None)
    }
    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print("Indexes built and saved.")
    return

# --------- Retrieval ----------
def load_indexes_and_metadata():
    if not METADATA_PATH.exists():
        raise RuntimeError("Metadata not found. Run build_indexes() first.")
    with open(METADATA_PATH, "r", encoding="utf-8") as f:
        metadata = json.load(f)
    text_index = None
    table_index = None
    if TEXT_INDEX_PATH.exists():
        text_index = faiss.read_index(str(TEXT_INDEX_PATH))
    if TABLE_INDEX_PATH.exists():
        table_index = faiss.read_index(str(TABLE_INDEX_PATH))
    embed_model = SentenceTransformer(EMBED_MODEL_NAME)
    return text_index, table_index, metadata, embed_model

def retrieve(query: str, top_k_text=TOP_K_TEXT, top_k_table=TOP_K_TABLE):
    text_index, table_index, metadata, embed_model = load_indexes_and_metadata()

    q_emb = embed_model.encode([query], convert_to_numpy=True)
    faiss.normalize_L2(q_emb)

    results = {"text": [], "table": []}

    if text_index is not None and len(metadata.get("paragraphs", []))>0:
        D, I = text_index.search(q_emb.astype("float32"), top_k_text)
        for score, idx in zip(D[0], I[0]):
            if idx < 0: continue
            para = metadata["paragraphs"][idx]
            results["text"].append({"score": float(score), "text": para, "index": int(idx)})

    if table_index is not None and len(metadata.get("table_rows", []))>0:
        D, I = table_index.search(q_emb.astype("float32"), top_k_table)
        for score, idx in zip(D[0], I[0]):
            if idx < 0: continue
            row = metadata["table_rows"][idx]
            results["table"].append({"score": float(score), "row_text": row["row_text"], "table_index": row.get("table_index"), "index": int(idx)})

    return results

# --------- Generation (quantized) ----------
def generate_with_llama_cpp(prompt: str, model_path: str, max_tokens: int = 256, temperature: float = 0.2) -> str:
    """
    Uses llama-cpp-python if available. Expects a GGML quantized model file path.
    """
    try:
        from llama_cpp import Llama
    except Exception as e:
        raise RuntimeError("llama-cpp-python not installed") from e

    llama = Llama(model_path=model_path, n_ctx=2048)
    resp = llama.create(prompt=prompt, max_tokens=max_tokens, temperature=temperature)
    return resp.get("choices", [{}])[0].get("text", "")

def generate_with_transformers(prompt: str, hf_model: str = "meta-llama/Llama-2-7b-chat-hf", max_tokens: int = 256, temperature: float = 0.2) -> str:
    """
    Fallback generator using Hugging Face transformers with bitsandbytes if available (GPU).
    Only use if you have a GPU and bitsandbytes configured.
    """
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline
    except Exception as e:
        raise RuntimeError("transformers not installed or unavailable") from e

    tokenizer = AutoTokenizer.from_pretrained(hf_model, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(hf_model, device_map="auto", load_in_8bit=True)
    gen_cfg = GenerationConfig(max_new_tokens=max_tokens, temperature=temperature)
    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0)
    out = pipe(prompt, max_new_tokens=max_tokens, do_sample=True, temperature=temperature)
    return out[0]["generated_text"][len(prompt):]

def generate_answer(query: str, retrievals: Dict, use_llama_cpp=USE_LLAMA_CPP) -> str:
    # Build a prompt with retrieved passages
    header = "You are an assistant that answers user questions using retrieved documents and table rows. Answer concisely and cite sources (text paragraphs or table rows by index). If you don't know, say you don't know.\n\n"
    prompt_parts = [header, f"User question: {query}\n\n"]
    # Add text context
    if retrievals.get("text"):
        prompt_parts.append("Top text passages:\n")
        for i, t in enumerate(retrievals["text"], start=1):
            prompt_parts.append(f"[Text#{t['index']}] (score={t['score']:.3f}): {t['text']}\n")
        prompt_parts.append("\n")
    # Add table context
    if retrievals.get("table"):
        prompt_parts.append("Top table rows:\n")
        for i, r in enumerate(retrievals["table"], start=1):
            prompt_parts.append(f"[Table#{r['index']}, table={r.get('table_index')}] (score={r['score']:.3f}): {r['row_text']}\n")
        prompt_parts.append("\n")
    # Instruction to answer
    prompt_parts.append("Using the retrieved information above, provide a direct answer to the user, and list references of which text/table items you used.\n\nAnswer:\n")
    prompt = "\n".join(prompt_parts)

    # Generate using quantized model if available
    if use_llama_cpp:
        try:
            print("Generating with llama-cpp (GGML quantized model):", LLAMA_CPP_MODEL_PATH)
            out = generate_with_llama_cpp(prompt, model_path=LLAMA_CPP_MODEL_PATH, max_tokens=512)
            return out.strip()
        except Exception as e:
            print("llama-cpp generation failed:", e)
            # fallback to transformers
    # Fallback
    print("Generating with transformers fallback (requires GPU for good perf)...")
    try:
        out = generate_with_transformers(prompt)
        return out.strip()
    except Exception as e:
        print("Transformers fallback failed:", e)
        return "Error: no generation model available in this environment."

# ---------- Main interactive ----------
def interactive_loop():
    print("Building indexes if not present...")
    if not METADATA_PATH.exists() or not TEXT_INDEX_PATH.exists() or not TABLE_INDEX_PATH.exists():
        build_indexes(JSON_PATH)
    else:
        print("Indexes already present.")

    print("Ready. Type a question (or 'exit').")
    while True:
        q = input("\n> ").strip()
        if not q: 
            continue
        if q.lower() in ("exit", "quit"):
            break
        retrievals = retrieve(q, top_k_text=TOP_K_TEXT, top_k_table=TOP_K_TABLE)
        print("\n--- Retrieval results ---")
        print("Top text results:")
        for t in retrievals["text"]:
            print(f"  [Text#{t['index']}] score={t['score']:.3f} -> {t['text'][:200]}...")
        print("\nTop table results:")
        for r in retrievals["table"]:
            print(f"  [Table#{r['index']}] score={r['score']:.3f} -> {r['row_text']}")

        print("\nGenerating answer (this may take a while)...")
        ans = generate_answer(q, retrievals)
        print("\n--- Generated answer ---\n")
        print(ans)
        print("\n--- End ---\n")

if __name__ == "__main__":
    interactive_loop()
